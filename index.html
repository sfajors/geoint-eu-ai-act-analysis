<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>European Union Artificial Intelligence Act</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <section class="header-container">
        <div class="statement-container">
          <div class="main-statement">
              <div class="first-statement">AI Regulation Under the Microscope: A GEOINT Deconstruct</div>
              <div class="second-statement">An examination of the Logical Fallacies within European Union's Artificial Intelligence Act</div>
          </div>
        </div>
    </section>
    <section class="collapsible-container">
    <section class="article-5">
        <button class="collapsible"><h1 class="article-title-5">Article 5: Prohibited Artificial Intelligence Practices</h1></button>
        <div class="content">
            <div class="article-text-5" style="margin-bottom: 1em">Article 5 outlines prohibited artificial intelligence practices and contains several logical fallacies, such as slippery slope, appeal to fear, false dilemma, and begging the question. However, the most prominent fallacy undermining its effectiveness is hasty generalization.</div>
            <div class="article-text-5" style="margin-bottom: 1em;">For instance, the prohibition against AI systems deploying subliminal or manipulative techniques broadly categorizes all such systems as harmful without considering the nuances or potential beneficial uses of the technology, such as in therapeutic settings. This overgeneralization can stifle innovation by banning applications that could responsibly use subliminal techniques for positive outcomes, such as mental health treatments or educational aids.</div>
            <div class="article-text-5" style="margin-bottom: 1em;">To support the statement about hasty generalization in Article 5, consider the following quote:</div>
            <div class="article-text-5" style="margin-bottom: 1em;">"the placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyond a person's consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behavior of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm."</div>
            <div class="article-text-5" style="margin-bottom: 1em;">This quote highlights the blanket prohibition of AI systems using subliminal techniques.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            The EU AI Act prohibits certain uses of artificial intelligence (AI). These include AI systems that manipulate people's decisions or exploit their vulnerabilities, systems that evaluate or classify people based on their social behavior or personal traits, and systems that predict a person's risk of committing a crime. The Act also bans AI systems that scrape facial images from the internet or CCTV footage, infer emotions in the workplace or educational institutions, and categorize people based on their biometric data. However, some exceptions are made for law enforcement purposes, such as searching for missing persons or preventing terrorist attacks.
        </div>
    </section>
    <section class="article-6">
        <button class="collapsible"><h1 class="article-title-6">Article 6: Classification for High-Risk AI Systems</h1></button>
        <div class="content">
            <div class="article-text-6" style="margin-bottom: 1em;">Article 6 classifies AI systems as high-risk and contains several fallacies, including appeal to authority, false dilemma, and slippery slope, that undermine its objectives; however, the most notable fallacy is hasty generalization.</div>
            <div class="article-text-6" style="margin-bottom: 1em;">The article broadly labels AI systems as high-risk if they are used for safety or fall under Union harmonization legislation. This generalization assumes all such AI systems are inherently dangerous because it doesn't consider their specific contexts or functionalities. By doing so, it implies that any AI system in these categories poses significant risks, regardless of its actual design or use. As a result, many AI systems could be unnecessarily classified as high-risk, which could stifle innovation and increase compliance costs without significantly improving fundamental rights.</div>
            <div class="article-text-6" style="margin-bottom: 1em;">To support the text about hasty generalization in Article 6, focus on the following specific section: </div>
            <div class="article-text-6" style="margin-bottom: 1em;">"the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered by the Union harmonisation legislation listed in Annex I."</div>
            <div class="article-text-6" style="margin-bottom: 1em;">This section highlights the broad classification of AI systems as high-risk based solely on their intended use as a safety component or their coverage under Union harmonisation legislation, without considering the specific context or functionality of each AI system.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article outlines how to classify high-risk AI systems. An AI system is considered high-risk if it is used as a safety component of a product, or if it is a product itself that is covered by EU legislation. These systems must undergo a third-party assessment before they can be sold or used. Some AI systems are always considered high-risk, unless they don't pose a significant risk to people's health, safety, or rights. Providers who believe their AI system isn't high-risk must document their assessment before selling or using it. The EU Commission will provide guidelines and examples of high-risk and non-high-risk AI systems. They can also add or remove conditions for high-risk classification based on evidence. Any changes must not decrease the level of protection for health, safety, and rights.
        </div>
    </section>
    <section class="article-14">
        <button class="collapsible"><h1 class="article-title-14">Article 14: Human Oversight</h1></button>
        <div class="content">
            <div class="article-text-14" style="margin-bottom: 1em;">Article 14 focuses on human oversight of high-risk AI systems and contains several logical fallacies, such as false dilemma, hasty generalization, slippery slope, circular reasoning, and appeal to traditions. However, the most prominent fallacy undermining its effectiveness is appeal to authority.</div>
            <div class="article-text-14" style="margin-bottom: 1em;">The article assumes that the inclusion of human-machine interface tools designed by experts will guarantee effective oversight. This reliance on expert design may lead to overconfidence in human oversight effectiveness, potentially overlooking the variability in human operators' expertise and the need for ongoing training and adaptation to specific use cases. As a result, this could undermine the goal of minimizing risks to health, safety, and fundamental rights.</div>
            <div class="article-text-14" style="margin-bottom: 1em;">To support the text about the appeal to authority in Article 14, focus on the following specific section:</div>
            <div class="article-text-14" style="margin-bottom: 1em;">"High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use."</div>
            <div class="article-text-14" style="margin-bottom: 1em;">This section assumes that the design and development of human-machine interface tools by experts will inherently ensure effective oversight, potentially leading to overconfidence in the effectiveness of these tools and overlooking the variability in human operators' expertise.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that high-risk AI systems must be designed in a way that allows humans to effectively oversee them. The goal of human oversight is to prevent or minimize risks to health, safety, or fundamental rights that may arise from using these systems. The oversight measures should match the risks and context of the AI system's use. These measures could be built into the system by the provider or implemented by the user. The AI system should be provided in a way that allows the overseer to understand its capabilities and limitations, detect and address issues, avoid over-reliance on the system, interpret its output, decide not to use it, or stop its operation. For certain high-risk AI systems, any action or decision based on the system's identification must be verified by at least two competent individuals.
        </div>
    </section>
    <section class="article-10">
        <button class="collapsible"><h1 class="article-title-10">Article 10: Data and Data Governance</h1></button>
        <div class="content">
            <div class="article-text-10" style="margin-bottom: 1em;">Article 10 focuses on data and data governance for high-risk AI systems and contains several logical fallacies, including appeal to authority, false dilemmas, slippery slope, circular reasoning, and appeal to tradition. The most prominent fallacy is hasty generalization. </div>
            <div class="article-text-10" style="margin-bottom: 1em;">The article assumes that applying data governance and management practices will uniformly ensure the quality and appropriateness of data sets for all high-risk AI systems. This oversimplification overlooks the complexities and variations in data quality and governance across different contexts and applications. As a result, inadequate data governance practices may fail to address specific challenges, potentially compromising the effectiveness and safety of high-risk AI systems and undermining the Act's goal of ensuring robust and context-specific data governance.</div>
            <div class="article-text-10" style="margin-bottom: 1em;">To support the text about hasty generalization in Article 10, focus on the following specific section:</div>
            <div class="article-text-10" style="margin-bottom: 1em;">"Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to be used."</div>
            <div class="article-text-10" style="margin-bottom: 1em;">This section assumes that applying general data governance and management practices will uniformly ensure the quality and appropriateness of data sets for all high-risk AI systems, without considering the specific complexities and variations in different contexts and applications.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that high-risk AI systems must be developed using high-quality data sets for training, validation, and testing. These data sets should be managed properly, considering factors like data collection processes, data preparation, potential biases, and data gaps. The data sets should be relevant, representative, error-free, and complete as much as possible. They should also consider the specific context in which the AI system will be used. In some cases, providers may process special categories of personal data to detect and correct biases, but they must follow strict conditions to protect individuals' rights and freedoms.
        </div>
    </section>
    <section class="article-15">
        <button class="collapsible"><h1 class="article-title-15">Article 15: Accuracy, Robustness, and Cybersecurity</h1></button>
        <div class="content">
            <div class="article-text-15" style="margin-bottom: 1em;">Article 15 focuses on accuracy, robustness, and cybersecurity for high-risk AI systems and contains several logical fallacies, such as false dilemma, hasty generalization, slippery slope, circular reasoning, and appeal to tradition, with the most notable being appeal to authority.</div>
            <div class="article-text-15" style="margin-bottom: 1em;">The article assumes that the involvement of the Commission and relevant authorities will inherently lead to the development of effective benchmarks and measurement methodologies. This reliance on authority without providing empirical evidence or clear criteria for success can result in overconfidence in the established benchmarks and measurement methodologies. Consequently, this may lead to inadequate or poorly designed standards, undermining the Act's goal of ensuring robust and accurate high-risk AI systems.</div>
            <div class="article-text-15" style="margin-bottom: 1em;">To support the text about the appeal to authority in Article 15, focus on the following specific section:</div>
            <div class="article-text-15" style="margin-bottom: 1em;">"To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out in paragraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholders and organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development of benchmarks and measurement methodologies."</div>
            <div class="article-text-15" style="margin-bottom: 1em;">This section assumes that the involvement of the Commission and relevant authorities will inherently lead to the development of effective benchmarks and measurement methodologies, without providing empirical evidence or clear criteria for success.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            The EU AI Act states that high-risk AI systems must be designed to be accurate, robust, and secure. They should perform consistently throughout their lifecycle. The Commission will work with relevant stakeholders to develop ways to measure these qualities. The accuracy of these AI systems should be declared in their instructions. These systems should be resilient to errors and faults, and should have backup plans in place. They should also be designed to reduce the risk of biased outputs. Finally, these systems should be secure against unauthorized third parties trying to exploit their vulnerabilities.
        </div>
    </section>
    <section class="article-8">
        <button class="collapsible"><h1 class="article-title-8">Article 8: Compliance with the Requirements</h1></button>
        <div class="content">
            <div class="article-text-8" style="margin-bottom: 1em;">Article 8 addresses compliance with the requirements for high-risk AI systems and contains several logical fallacies, including false dilemma, hasty generalization, circular reasoning, and appeal to tradition. The most prominent fallacy is appeal to authority.</div>
            <div class="article-text-8" style="margin-bottom: 1em;">The article assumes that adherence to the "generally acknowledged state of the art" in AI and AI-related technologies inherently ensures compliance and safety. This reliance on current standards and practices, without critically examining their adequacy or relevance, can lead to complacency. Consequently, it may ignore emerging risks or novel challenges that existing standards do not yet address, undermining the Act's goal of ensuring robust and forward-looking regulation of high-risk AI systems.</div>
            <div class="article-text-8" style="margin-bottom: 1em;">To support the text about the appeal to authority in Article 8, focus on the following specific section:</div>
            <div class="article-text-8" style="margin-bottom: 1em;">"High-risk AI systems shall comply with the requirements laid down in this Section, taking into account their intended purpose as well as the generally acknowledged state of the art on AI and AI-related technologies."</div>
            <div class="article-text-8" style="margin-bottom: 1em;">This section assumes that adherence to the "generally acknowledged state of the art" in AI and AI-related technologies inherently ensures compliance and safety, without critically examining the adequacy or relevance of these current standards and practices.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that high-risk AI systems must meet certain standards, considering their purpose and the current state of AI technology. If a product contains an AI system, the providers must ensure it meets all relevant EU regulations. To avoid unnecessary duplication and extra work, providers can choose to incorporate necessary testing, reporting, and documentation into existing procedures required by EU legislation.
        </div>
    </section>
    <section class="article-16">
        <button class="collapsible"><h1 class="article-title-16">Article 16: Obligations of Providers of High-Risk AI Systems</h1></button>
        <div class="content">
            <div class="article-text-16" style="margin-bottom: 1em;">Article 16 outlines the obligations of providers of high-risk AI systems and contains multiple fallacies, such as false dilemma, hasty generalization, circular reasoning, and appeal to tradition. The most notable fallacy is the appeal to authority.</div>
            <div class="article-text-16" style="margin-bottom: 1em;">The article assumes that simply complying with the requirements set out in Section 2 will inherently ensure the safety and effectiveness of high-risk AI systems. This reliance on the authority of these requirements, without critically evaluating their adequacy or effectiveness in all scenarios, can lead to a false sense of security. This approach may overlook specific contexts or emerging risks that the established requirements do not fully address, undermining the Act's goal of ensuring comprehensive and adaptive safety measures for high-risk AI systems.</div>
            <div class="article-text-16" style="margin-bottom: 1em;">To support the text about the appeal to authority in Article 16, focus on the following specific section:</div>
            <div class="article-text-16" style="margin-bottom: 1em;">"ensure that their high-risk AI systems are compliant with the requirements set out in Section 2;"</div>
            <div class="article-text-16" style="margin-bottom: 1em;">This section assumes that simply complying with the requirements in Section 2 will inherently ensure the safety and effectiveness of high-risk AI systems, relying on the authority of these requirements without critically evaluating their adequacy or effectiveness in all scenarios.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that companies providing high-risk AI systems must follow certain rules. They must make sure their AI systems meet specific standards and display their contact information on the product or its packaging. They need to have a quality management system and keep certain documents and logs. Before selling or using the AI system, they must have it checked for compliance with regulations. They also need to mark the product with a CE marking to show it meets EU standards. They must register the product, fix any issues, provide necessary information, and prove it meets standards if asked by authorities. The AI system must also be accessible according to EU directives.
        </div>
    </section>
    <section class="article-9">
        <button class="collapsible"><h1 class="article-title-9">Article 9: Risk Management System</h1></button>
        <div class="content">
            <div class="article-text-9" style="margin-bottom: 1em;">Article 9 outlines the risk management system for high-risk AI systems and contains many fallacies, including circular reasoning, false dilemma, and appeal to authority. The most evident fallacy is hasty generalization.</div>
            <div class="article-text-9" style="margin-bottom: 1em;">The article implies that a single risk management system can be universally applied to all high-risk AI systems without accounting for the specific needs, contexts, and risks associated with different types of AI systems. This overgeneralization can lead to a one-size-fits-all approach, potentially overlooking unique risks and requirements of different AI systems, thus undermining the Act's goal of ensuring comprehensive and tailored risk management for each high-risk AI system.</div>
            <div class="article-text-9" style="margin-bottom: 1em;">To support the text about hasty generalization in Article 9, focus on the following specific section:</div>
            <div class="article-text-9" style="margin-bottom: 1em;">"A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems."</div>
            <div class="article-text-9" style="margin-bottom: 1em;">This section implies that a single risk management system can be universally applied to all high-risk AI systems without accounting for the specific needs, contexts, and risks associated with different types of AI systems.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            The EU AI Act requires a risk management system for high-risk AI systems. This system should be a continuous process throughout the AI's lifecycle, regularly reviewed and updated. It should identify and analyze potential risks to health, safety, or fundamental rights, estimate and evaluate these risks, and adopt measures to manage them. These measures should balance minimizing risks and fulfilling requirements. The system should also ensure that any remaining risk is acceptable and that measures are in place to eliminate or reduce risks as much as possible. High-risk AI systems should be tested to identify the best risk management measures and to ensure they work as intended. The system should also consider whether the AI could negatively impact people under 18 or other vulnerable groups.
        </div>
    </section>
    <section class="article-12">
        <button class="collapsible"><h1 class="article-title-12">Article 12: Record-Keeping</h1></button>
        <div class="content">
            <div class="article-text-12" style="margin-bottom: 1em;">Article 12 focuses on record-keeping for high-risk AI systems and contains several fallacies, such as false dilemma, appeal to authority, and slippery slope. However, the most persistent fallacy is hasty generalization.</div>
            <div class="article-text-12" style="margin-bottom: 1em;">The article assumes that all high-risk AI systems can feasibly implement automatic logging throughout their entire lifetime without considering the potential technical and operational challenges specific to different types of AI systems. This hasty generalization may lead to unrealistic expectations and potential non-compliance if certain AI systems face significant hurdles in implementing such comprehensive logging. This undermines the Act's aim of ensuring practical and achievable requirements for all high-risk AI systems.</div>
            <div class="article-text-12" style="margin-bottom: 1em;">To support the text about hasty generalization in Article 12, focus on the following specific section:</div>
            <div class="article-text-12" style="margin-bottom: 1em;">“High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system."</div>
            <div class="article-text-12" style="margin-bottom: 1em;">This section assumes that all high-risk AI systems can feasibly implement automatic logging throughout their entire lifetime, without considering the potential technical and operational challenges specific to different types of AI systems.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that high-risk AI systems must have the ability to automatically record events throughout their lifespan. This is to ensure that the system's actions can be traced back, especially in situations where the AI might pose a risk or undergo significant changes. The system should also record details like when it was used, the database it checked data against, the data that matched, and who verified the results. This is to ensure accountability and safety in the use of high-risk AI systems.
        </div>
    </section>
    <section class="article-13">
        <button class="collapsible"><h1 class="article-title-13">Article 13: Transparency and Provisions of Information</h1></button>
        <div class="content">
            <div class="article-text-13" style="margin-bottom: 1em;">Article 13 focuses on transparency and the provision of information to deployers and contains multiple fallacies, including false dilemma, appeal to authority, and slippery slope. The most evident fallacy is overgeneralization. </div>
            <div class="article-text-13" style="margin-bottom: 1em;">The article assumes that all high-risk AI systems can be made sufficiently transparent for any deployer to understand and use appropriately, without considering the variability in the complexity of AI systems and the technical expertise of different deployers. This overgeneralization can lead to impractical expectations and potential non-compliance if some high-risk AI systems are inherently complex or if deployers lack the necessary expertise to interpret the outputs, undermining the Act's goal of effective and practical transparency requirements.</div>
            <div class="article-text-13" style="margin-bottom: 1em;">To support the text about overgeneralization in Article 13, focus on the following specific section:</div>
            <div class="article-text-13" style="margin-bottom: 1em;">“High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system's output and use it appropriately."</div>
            <div class="article-text-13" style="margin-bottom: 1em;">This section assumes that all high-risk AI systems can be made sufficiently transparent for any deployer to understand and use appropriately, without considering the variability in the complexity of AI systems and the technical expertise of different deployers.</div>
        </div>
        <button class="summary-reveal">Summary</button>
        <div class="summary">
            This article states that high-risk AI systems must be designed to be transparent, so that those using them can understand and use them correctly. They must come with clear instructions, including information about the provider, the system's capabilities and limitations, and any potential risks. The instructions should also explain how to interpret the system's output, any pre-determined changes to the system, and how to maintain it. If relevant, they should also describe how to collect, store and interpret data logs.
        </div>
    </section>
    </section>
    <section class="quiz-container">
        <h2>Were You Paying Attention?</h2>
        <div class="question-container"></div>
        <button id="next-btn" class="next-question">Next Question</button>
        <button id="restart-btn" style="display:none;">Restart Quiz</button>
        <div id="result"></div>
    </section>    

    <script src="script.js"></script>
</body>
</html>
